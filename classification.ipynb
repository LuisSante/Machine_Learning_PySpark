{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __1. Setup Spark and load other libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "np.random.seed(60)\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator \n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .appName(\"Crime_Classification\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __2. Data Extraction__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_dataset = pd.read_csv('train.csv')\n",
    "crime_dataset['Latlong'] = crime_dataset['X']*crime_dataset['Y']\n",
    "crime_dataset.drop(['X','Y','Dates'],axis='columns',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Descript</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>PdDistrict</th>\n",
       "      <th>Resolution</th>\n",
       "      <th>Address</th>\n",
       "      <th>Latlong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WARRANTS</td>\n",
       "      <td>WARRANT ARREST</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>ARREST, BOOKED</td>\n",
       "      <td>OAK ST / LAGUNA ST</td>\n",
       "      <td>-4624.588916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OTHER OFFENSES</td>\n",
       "      <td>TRAFFIC VIOLATION ARREST</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>ARREST, BOOKED</td>\n",
       "      <td>OAK ST / LAGUNA ST</td>\n",
       "      <td>-4624.588916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OTHER OFFENSES</td>\n",
       "      <td>TRAFFIC VIOLATION ARREST</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>ARREST, BOOKED</td>\n",
       "      <td>VANNESS AV / GREENWICH ST</td>\n",
       "      <td>-4627.691645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LARCENY/THEFT</td>\n",
       "      <td>GRAND THEFT FROM LOCKED AUTO</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1500 Block of LOMBARD ST</td>\n",
       "      <td>-4627.847257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LARCENY/THEFT</td>\n",
       "      <td>GRAND THEFT FROM LOCKED AUTO</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>PARK</td>\n",
       "      <td>NONE</td>\n",
       "      <td>100 Block of BRODERICK ST</td>\n",
       "      <td>-4624.699819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878044</th>\n",
       "      <td>ROBBERY</td>\n",
       "      <td>ROBBERY ON THE STREET WITH A GUN</td>\n",
       "      <td>Monday</td>\n",
       "      <td>TARAVAL</td>\n",
       "      <td>NONE</td>\n",
       "      <td>FARALLONES ST / CAPITOL AV</td>\n",
       "      <td>-4618.426865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878045</th>\n",
       "      <td>LARCENY/THEFT</td>\n",
       "      <td>GRAND THEFT FROM LOCKED AUTO</td>\n",
       "      <td>Monday</td>\n",
       "      <td>INGLESIDE</td>\n",
       "      <td>NONE</td>\n",
       "      <td>600 Block of EDNA ST</td>\n",
       "      <td>-4620.177499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878046</th>\n",
       "      <td>LARCENY/THEFT</td>\n",
       "      <td>GRAND THEFT FROM LOCKED AUTO</td>\n",
       "      <td>Monday</td>\n",
       "      <td>SOUTHERN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>5TH ST / FOLSOM ST</td>\n",
       "      <td>-4624.432596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878047</th>\n",
       "      <td>VANDALISM</td>\n",
       "      <td>MALICIOUS MISCHIEF, VANDALISM OF VEHICLES</td>\n",
       "      <td>Monday</td>\n",
       "      <td>SOUTHERN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>TOWNSEND ST / 2ND ST</td>\n",
       "      <td>-4623.988577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878048</th>\n",
       "      <td>FORGERY/COUNTERFEITING</td>\n",
       "      <td>CHECKS, FORGERY (FELONY)</td>\n",
       "      <td>Monday</td>\n",
       "      <td>BAYVIEW</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1800 Block of NEWCOMB AV</td>\n",
       "      <td>-4618.965598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>878049 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Category                                   Descript  \\\n",
       "0                     WARRANTS                             WARRANT ARREST   \n",
       "1               OTHER OFFENSES                   TRAFFIC VIOLATION ARREST   \n",
       "2               OTHER OFFENSES                   TRAFFIC VIOLATION ARREST   \n",
       "3                LARCENY/THEFT               GRAND THEFT FROM LOCKED AUTO   \n",
       "4                LARCENY/THEFT               GRAND THEFT FROM LOCKED AUTO   \n",
       "...                        ...                                        ...   \n",
       "878044                 ROBBERY           ROBBERY ON THE STREET WITH A GUN   \n",
       "878045           LARCENY/THEFT               GRAND THEFT FROM LOCKED AUTO   \n",
       "878046           LARCENY/THEFT               GRAND THEFT FROM LOCKED AUTO   \n",
       "878047               VANDALISM  MALICIOUS MISCHIEF, VANDALISM OF VEHICLES   \n",
       "878048  FORGERY/COUNTERFEITING                   CHECKS, FORGERY (FELONY)   \n",
       "\n",
       "        DayOfWeek PdDistrict      Resolution                     Address  \\\n",
       "0       Wednesday   NORTHERN  ARREST, BOOKED          OAK ST / LAGUNA ST   \n",
       "1       Wednesday   NORTHERN  ARREST, BOOKED          OAK ST / LAGUNA ST   \n",
       "2       Wednesday   NORTHERN  ARREST, BOOKED   VANNESS AV / GREENWICH ST   \n",
       "3       Wednesday   NORTHERN            NONE    1500 Block of LOMBARD ST   \n",
       "4       Wednesday       PARK            NONE   100 Block of BRODERICK ST   \n",
       "...           ...        ...             ...                         ...   \n",
       "878044     Monday    TARAVAL            NONE  FARALLONES ST / CAPITOL AV   \n",
       "878045     Monday  INGLESIDE            NONE        600 Block of EDNA ST   \n",
       "878046     Monday   SOUTHERN            NONE          5TH ST / FOLSOM ST   \n",
       "878047     Monday   SOUTHERN            NONE        TOWNSEND ST / 2ND ST   \n",
       "878048     Monday    BAYVIEW            NONE    1800 Block of NEWCOMB AV   \n",
       "\n",
       "            Latlong  \n",
       "0      -4624.588916  \n",
       "1      -4624.588916  \n",
       "2      -4627.691645  \n",
       "3      -4627.847257  \n",
       "4      -4624.699819  \n",
       "...             ...  \n",
       "878044 -4618.426865  \n",
       "878045 -4620.177499  \n",
       "878046 -4624.432596  \n",
       "878047 -4623.988577  \n",
       "878048 -4618.965598  \n",
       "\n",
       "[878049 rows x 7 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crime_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = crime_dataset.to_csv('preproccesing_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __3.Define Structure to build Pipeline__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv')\\\n",
    "          .option('header','true')\\\n",
    "          .option('inferSchema', 'true')\\\n",
    "          .option('timestamp', 'true')\\\n",
    "          .load('preproccesing_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Category',\n",
       " 'Descript',\n",
       " 'DayOfWeek',\n",
       " 'PdDistrict',\n",
       " 'Resolution',\n",
       " 'Address',\n",
       " 'Latlong']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe Structure\n",
      "----------------------------------\n",
      "root\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Descript: string (nullable = true)\n",
      " |-- DayOfWeek: string (nullable = true)\n",
      " |-- PdDistrict: string (nullable = true)\n",
      " |-- Resolution: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Latlong: double (nullable = true)\n",
      "\n",
      "None\n",
      " \n",
      "Dataframe preview\n",
      "+--------------+--------------------+---------+----------+--------------+--------------------+------------------+\n",
      "|      Category|            Descript|DayOfWeek|PdDistrict|    Resolution|             Address|           Latlong|\n",
      "+--------------+--------------------+---------+----------+--------------+--------------------+------------------+\n",
      "|      WARRANTS|      WARRANT ARREST|Wednesday|  NORTHERN|ARREST, BOOKED|  OAK ST / LAGUNA ST|-4624.588915745816|\n",
      "|OTHER OFFENSES|TRAFFIC VIOLATION...|Wednesday|  NORTHERN|ARREST, BOOKED|  OAK ST / LAGUNA ST|-4624.588915745816|\n",
      "|OTHER OFFENSES|TRAFFIC VIOLATION...|Wednesday|  NORTHERN|ARREST, BOOKED|VANNESS AV / GREE...|-4627.691645315983|\n",
      "| LARCENY/THEFT|GRAND THEFT FROM ...|Wednesday|  NORTHERN|          NONE|1500 Block of LOM...|-4627.847257159713|\n",
      "| LARCENY/THEFT|GRAND THEFT FROM ...|Wednesday|      PARK|          NONE|100 Block of BROD...|-4624.699819172749|\n",
      "+--------------+--------------------+---------+----------+--------------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      " \n",
      "----------------------------------\n",
      "Total number of rows 878049\n"
     ]
    }
   ],
   "source": [
    "print('Dataframe Structure')\n",
    "print('----------------------------------')\n",
    "print(df.printSchema())\n",
    "print(' ')\n",
    "print('Dataframe preview')\n",
    "print(df.show(5))\n",
    "print(' ')\n",
    "print('----------------------------------')\n",
    "print('Total number of rows', df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique value of Resolution: 17\n",
      " \n",
      "Top 12 Crime Resolution\n",
      "+--------------------+----------+\n",
      "|          Resolution|totalValue|\n",
      "+--------------------+----------+\n",
      "|                NONE|    526790|\n",
      "|      ARREST, BOOKED|    206403|\n",
      "|       ARREST, CITED|     77004|\n",
      "|             LOCATED|     17101|\n",
      "|   PSYCHOPATHIC CASE|     14534|\n",
      "|           UNFOUNDED|      9585|\n",
      "|     JUVENILE BOOKED|      5564|\n",
      "|COMPLAINANT REFUS...|      3976|\n",
      "|DISTRICT ATTORNEY...|      3934|\n",
      "|      NOT PROSECUTED|      3714|\n",
      "|      JUVENILE CITED|      3332|\n",
      "|PROSECUTED BY OUT...|      2504|\n",
      "+--------------------+----------+\n",
      "only showing top 12 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def top_n_list(df,name_column, N):\n",
    "    print(\"Total number of unique value of\"+' '+name_column+''+':'+' '+str(df.select(name_column).distinct().count()))\n",
    "    print(' ')\n",
    "    print('Top'+' '+str(N)+' '+'Crime'+' '+name_column)\n",
    "    df.groupBy(name_column).count().withColumnRenamed('count','totalValue').orderBy(col('totalValue').desc()).show(N)\n",
    "    \n",
    "    \n",
    "top_n_list(df, 'Resolution',12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __4. Partition the dataset into Training and Test dataset__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator \n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando una lista de columnas categÃ³ricas, excluyendo 'Category'\n",
    "categorical_columns = ['Descript','DayOfWeek','PdDistrict','Resolution','Address']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(df) for column in categorical_columns]\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[indexer.getOutputCol() for indexer in indexers],\n",
    "                        outputCols=[column+\"_ohe\" for column in categorical_columns])\n",
    "\n",
    "# Agregar un StringIndexer solo para 'Category'\n",
    "category_indexer = StringIndexer(inputCol=\"Category\", outputCol=\"Category_index\").fit(df)\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + [encoder, category_indexer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating feature_columns\n",
    "feature_columns = [column+\"_ohe\" for column in categorical_columns] + ['Latlong']\n",
    "\n",
    "(train_data, test_data) = df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n",
    "\n",
    "# Updating labelCol in LogisticRegression\n",
    "lr = LogisticRegression(featuresCol='scaledFeatures', labelCol='Category_index', maxIter=10)\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, scaler, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|Category_index|prediction|\n",
      "+--------------+----------+\n",
      "|          27.0|      27.0|\n",
      "|          27.0|      27.0|\n",
      "|          27.0|      27.0|\n",
      "|          27.0|      27.0|\n",
      "|          27.0|      27.0|\n",
      "|          27.0|      27.0|\n",
      "|          27.0|      27.0|\n",
      "|          27.0|      27.0|\n",
      "|          27.0|      27.0|\n",
      "|          27.0|      27.0|\n",
      "|          27.0|      27.0|\n",
      "|          27.0|      27.0|\n",
      "|          27.0|      27.0|\n",
      "|          27.0|      27.0|\n",
      "|          27.0|      27.0|\n",
      "|          27.0|      27.0|\n",
      "|          27.0|      27.0|\n",
      "|          27.0|      27.0|\n",
      "|          27.0|      27.0|\n",
      "|          27.0|      27.0|\n",
      "+--------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test_data)\n",
    "predictions = predictions.select('Category_index', 'prediction')\n",
    "#predictions.select('Category_index', 'prediction')\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Category_index: double (nullable = false)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------Accuracy-----------------------------\n",
      " \n",
      "               accuracy:0.9968266523455473\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Category_index\").setPredictionCol(\"prediction\").evaluate(predictions)\n",
    "print(' ')\n",
    "print('--------------------------Accuracy-----------------------------')\n",
    "print(' ')\n",
    "print('               accuracy:{}'.format(evaluator))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-----+\n",
      "|Category_index|prediction|count|\n",
      "+--------------+----------+-----+\n",
      "|          22.0|       5.0|  234|\n",
      "|          36.0|      36.0|   42|\n",
      "|           7.0|       7.0|12695|\n",
      "|          35.0|      35.0|   39|\n",
      "|          12.0|      12.0| 4959|\n",
      "|           3.0|      23.0|   90|\n",
      "|           1.0|       1.0|37641|\n",
      "|          31.0|      31.0|  134|\n",
      "|          17.0|       1.0|    1|\n",
      "|          10.0|      10.0| 7813|\n",
      "|          28.0|      28.0|  377|\n",
      "|          14.0|      14.0| 3052|\n",
      "|          27.0|      27.0|  441|\n",
      "|          21.0|      21.0| 1245|\n",
      "|          17.0|      17.0| 2205|\n",
      "|          26.0|      26.0|  556|\n",
      "|           2.0|       2.0|27892|\n",
      "|          24.0|      24.0|  644|\n",
      "|          23.0|      23.0|  683|\n",
      "|          19.0|      19.0| 1272|\n",
      "+--------------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `label` cannot be resolved. Did you mean one of the following? [`prediction`, `Category_index`].; line 1 pos 19;\n'Filter ((prediction#2848 = cast(0 as double)) AND ('label = prediction#2848))\n+- Project [Category_index#1877, prediction#2848]\n   +- Project [Category#1205, Descript#1206, DayOfWeek#1207, PdDistrict#1208, Resolution#1209, Address#1210, Latlong#1211, Descript_index#1720, DayOfWeek_index#1735, PdDistrict_index#1751, Resolution_index#1768, Address_index#1786, Descript_ohe#1822, DayOfWeek_ohe#1823, PdDistrict_ohe#1824, Resolution_ohe#1825, Address_ohe#1826, Category_index#1877, features#2745, scaledFeatures#2768, rawPrediction#2794, probability#2819, UDF(rawPrediction#2794) AS prediction#2848]\n      +- Project [Category#1205, Descript#1206, DayOfWeek#1207, PdDistrict#1208, Resolution#1209, Address#1210, Latlong#1211, Descript_index#1720, DayOfWeek_index#1735, PdDistrict_index#1751, Resolution_index#1768, Address_index#1786, Descript_ohe#1822, DayOfWeek_ohe#1823, PdDistrict_ohe#1824, Resolution_ohe#1825, Address_ohe#1826, Category_index#1877, features#2745, scaledFeatures#2768, rawPrediction#2794, UDF(rawPrediction#2794) AS probability#2819]\n         +- Project [Category#1205, Descript#1206, DayOfWeek#1207, PdDistrict#1208, Resolution#1209, Address#1210, Latlong#1211, Descript_index#1720, DayOfWeek_index#1735, PdDistrict_index#1751, Resolution_index#1768, Address_index#1786, Descript_ohe#1822, DayOfWeek_ohe#1823, PdDistrict_ohe#1824, Resolution_ohe#1825, Address_ohe#1826, Category_index#1877, features#2745, scaledFeatures#2768, UDF(scaledFeatures#2768) AS rawPrediction#2794]\n            +- Project [Category#1205, Descript#1206, DayOfWeek#1207, PdDistrict#1208, Resolution#1209, Address#1210, Latlong#1211, Descript_index#1720, DayOfWeek_index#1735, PdDistrict_index#1751, Resolution_index#1768, Address_index#1786, Descript_ohe#1822, DayOfWeek_ohe#1823, PdDistrict_ohe#1824, Resolution_ohe#1825, Address_ohe#1826, Category_index#1877, features#2745, UDF(features#2745) AS scaledFeatures#2768]\n               +- Project [Category#1205, Descript#1206, DayOfWeek#1207, PdDistrict#1208, Resolution#1209, Address#1210, Latlong#1211, Descript_index#1720, DayOfWeek_index#1735, PdDistrict_index#1751, Resolution_index#1768, Address_index#1786, Descript_ohe#1822, DayOfWeek_ohe#1823, PdDistrict_ohe#1824, Resolution_ohe#1825, Address_ohe#1826, Category_index#1877, UDF(struct(Descript_ohe, Descript_ohe#1822, DayOfWeek_ohe, DayOfWeek_ohe#1823, PdDistrict_ohe, PdDistrict_ohe#1824, Resolution_ohe, Resolution_ohe#1825, Address_ohe, Address_ohe#1826, Latlong, Latlong#1211)) AS features#2745]\n                  +- Sample 0.7, 1.0, false, 7230665535325484349\n                     +- Sort [Category#1205 ASC NULLS FIRST, Descript#1206 ASC NULLS FIRST, DayOfWeek#1207 ASC NULLS FIRST, PdDistrict#1208 ASC NULLS FIRST, Resolution#1209 ASC NULLS FIRST, Address#1210 ASC NULLS FIRST, Latlong#1211 ASC NULLS FIRST, Descript_index#1720 ASC NULLS FIRST, DayOfWeek_index#1735 ASC NULLS FIRST, PdDistrict_index#1751 ASC NULLS FIRST, Resolution_index#1768 ASC NULLS FIRST, Address_index#1786 ASC NULLS FIRST, Descript_ohe#1822 ASC NULLS FIRST, DayOfWeek_ohe#1823 ASC NULLS FIRST, PdDistrict_ohe#1824 ASC NULLS FIRST, Resolution_ohe#1825 ASC NULLS FIRST, Address_ohe#1826 ASC NULLS FIRST, Category_index#1877 ASC NULLS FIRST], false\n                        +- Project [Category#1205, Descript#1206, DayOfWeek#1207, PdDistrict#1208, Resolution#1209, Address#1210, Latlong#1211, Descript_index#1720, DayOfWeek_index#1735, PdDistrict_index#1751, Resolution_index#1768, Address_index#1786, Descript_ohe#1822, DayOfWeek_ohe#1823, PdDistrict_ohe#1824, Resolution_ohe#1825, Address_ohe#1826, UDF(cast(Category#1205 as string)) AS Category_index#1877]\n                           +- Project [Category#1205, Descript#1206, DayOfWeek#1207, PdDistrict#1208, Resolution#1209, Address#1210, Latlong#1211, Descript_index#1720, DayOfWeek_index#1735, PdDistrict_index#1751, Resolution_index#1768, Address_index#1786, UDF(cast(Descript_index#1720 as double), 0) AS Descript_ohe#1822, UDF(cast(DayOfWeek_index#1735 as double), 1) AS DayOfWeek_ohe#1823, UDF(cast(PdDistrict_index#1751 as double), 2) AS PdDistrict_ohe#1824, UDF(cast(Resolution_index#1768 as double), 3) AS Resolution_ohe#1825, UDF(cast(Address_index#1786 as double), 4) AS Address_ohe#1826]\n                              +- Project [Category#1205, Descript#1206, DayOfWeek#1207, PdDistrict#1208, Resolution#1209, Address#1210, Latlong#1211, Descript_index#1720, DayOfWeek_index#1735, PdDistrict_index#1751, Resolution_index#1768, UDF(cast(Address#1210 as string)) AS Address_index#1786]\n                                 +- Project [Category#1205, Descript#1206, DayOfWeek#1207, PdDistrict#1208, Resolution#1209, Address#1210, Latlong#1211, Descript_index#1720, DayOfWeek_index#1735, PdDistrict_index#1751, UDF(cast(Resolution#1209 as string)) AS Resolution_index#1768]\n                                    +- Project [Category#1205, Descript#1206, DayOfWeek#1207, PdDistrict#1208, Resolution#1209, Address#1210, Latlong#1211, Descript_index#1720, DayOfWeek_index#1735, UDF(cast(PdDistrict#1208 as string)) AS PdDistrict_index#1751]\n                                       +- Project [Category#1205, Descript#1206, DayOfWeek#1207, PdDistrict#1208, Resolution#1209, Address#1210, Latlong#1211, Descript_index#1720, UDF(cast(DayOfWeek#1207 as string)) AS DayOfWeek_index#1735]\n                                          +- Project [Category#1205, Descript#1206, DayOfWeek#1207, PdDistrict#1208, Resolution#1209, Address#1210, Latlong#1211, UDF(cast(Descript#1206 as string)) AS Descript_index#1720]\n                                             +- Relation [Category#1205,Descript#1206,DayOfWeek#1207,PdDistrict#1208,Resolution#1209,Address#1210,Latlong#1211] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Usuario\\Documents\\Universidad\\BigData\\Crime-Classification-using-PySpark\\classification.ipynb Celda 24\u001b[0m in \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Universidad/BigData/Crime-Classification-using-PySpark/classification.ipynb#X65sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m predictions\u001b[39m.\u001b[39mgroupBy(\u001b[39m'\u001b[39m\u001b[39mCategory_index\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mprediction\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mcount()\u001b[39m.\u001b[39mshow()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Universidad/BigData/Crime-Classification-using-PySpark/classification.ipynb#X65sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Calculate the elements of the confusion matrix\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Universidad/BigData/Crime-Classification-using-PySpark/classification.ipynb#X65sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m TN \u001b[39m=\u001b[39m predictions\u001b[39m.\u001b[39;49mfilter(\u001b[39m'\u001b[39;49m\u001b[39mprediction = 0 AND label = prediction\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39mcount()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Universidad/BigData/Crime-Classification-using-PySpark/classification.ipynb#X65sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m TP \u001b[39m=\u001b[39m predictions\u001b[39m.\u001b[39mfilter(\u001b[39m'\u001b[39m\u001b[39mprediction = 1 AND label = prediction\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mcount()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Universidad/BigData/Crime-Classification-using-PySpark/classification.ipynb#X65sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m FN \u001b[39m=\u001b[39m predictions\u001b[39m.\u001b[39mfilter(\u001b[39m'\u001b[39m\u001b[39mprediction = 0 AND label = 1\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mcount()\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:3136\u001b[0m, in \u001b[0;36mDataFrame.filter\u001b[1;34m(self, condition)\u001b[0m\n\u001b[0;32m   3080\u001b[0m \u001b[39m\"\"\"Filters rows using the given condition.\u001b[39;00m\n\u001b[0;32m   3081\u001b[0m \n\u001b[0;32m   3082\u001b[0m \u001b[39m:func:`where` is an alias for :func:`filter`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3133\u001b[0m \u001b[39m+---+-----+\u001b[39;00m\n\u001b[0;32m   3134\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3135\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(condition, \u001b[39mstr\u001b[39m):\n\u001b[1;32m-> 3136\u001b[0m     jdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mfilter(condition)\n\u001b[0;32m   3137\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(condition, Column):\n\u001b[0;32m   3138\u001b[0m     jdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jdf\u001b[39m.\u001b[39mfilter(condition\u001b[39m.\u001b[39m_jc)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `label` cannot be resolved. Did you mean one of the following? [`prediction`, `Category_index`].; line 1 pos 19;\n'Filter ((prediction#2848 = cast(0 as double)) AND ('label = prediction#2848))\n+- Project [Category_index#1877, prediction#2848]\n   +- Project [Category#1205, Descript#1206, DayOfWeek#1207, PdDistrict#1208, Resolution#1209, Address#1210, Latlong#1211, Descript_index#1720, DayOfWeek_index#1735, PdDistrict_index#1751, Resolution_index#1768, Address_index#1786, Descript_ohe#1822, DayOfWeek_ohe#1823, PdDistrict_ohe#1824, Resolution_ohe#1825, Address_ohe#1826, Category_index#1877, features#2745, scaledFeatures#2768, rawPrediction#2794, probability#2819, UDF(rawPrediction#2794) AS prediction#2848]\n      +- Project [Category#1205, Descript#1206, DayOfWeek#1207, PdDistrict#1208, Resolution#1209, Address#1210, Latlong#1211, Descript_index#1720, DayOfWeek_index#1735, PdDistrict_index#1751, Resolution_index#1768, Address_index#1786, Descript_ohe#1822, DayOfWeek_ohe#1823, PdDistrict_ohe#1824, Resolution_ohe#1825, Address_ohe#1826, Category_index#1877, features#2745, scaledFeatures#2768, rawPrediction#2794, UDF(rawPrediction#2794) AS probability#2819]\n         +- Project [Category#1205, Descript#1206, DayOfWeek#1207, PdDistrict#1208, Resolution#1209, Address#1210, Latlong#1211, Descript_index#1720, DayOfWeek_index#1735, PdDistrict_index#1751, Resolution_index#1768, Address_index#1786, Descript_ohe#1822, DayOfWeek_ohe#1823, PdDistrict_ohe#1824, Resolution_ohe#1825, Address_ohe#1826, Category_index#1877, features#2745, scaledFeatures#2768, UDF(scaledFeatures#2768) AS rawPrediction#2794]\n            +- Project [Category#1205, Descript#1206, DayOfWeek#1207, PdDistrict#1208, Resolution#1209, Address#1210, Latlong#1211, Descript_index#1720, DayOfWeek_index#1735, PdDistrict_index#1751, Resolution_index#1768, Address_index#1786, Descript_ohe#1822, DayOfWeek_ohe#1823, PdDistrict_ohe#1824, Resolution_ohe#1825, Address_ohe#1826, Category_index#1877, features#2745, UDF(features#2745) AS scaledFeatures#2768]\n               +- Project [Category#1205, Descript#1206, DayOfWeek#1207, PdDistrict#1208, Resolution#1209, Address#1210, Latlong#1211, Descript_index#1720, DayOfWeek_index#1735, PdDistrict_index#1751, Resolution_index#1768, Address_index#1786, Descript_ohe#1822, DayOfWeek_ohe#1823, PdDistrict_ohe#1824, Resolution_ohe#1825, Address_ohe#1826, Category_index#1877, UDF(struct(Descript_ohe, Descript_ohe#1822, DayOfWeek_ohe, DayOfWeek_ohe#1823, PdDistrict_ohe, PdDistrict_ohe#1824, Resolution_ohe, Resolution_ohe#1825, Address_ohe, Address_ohe#1826, Latlong, Latlong#1211)) AS features#2745]\n                  +- Sample 0.7, 1.0, false, 7230665535325484349\n                     +- Sort [Category#1205 ASC NULLS FIRST, Descript#1206 ASC NULLS FIRST, DayOfWeek#1207 ASC NULLS FIRST, PdDistrict#1208 ASC NULLS FIRST, Resolution#1209 ASC NULLS FIRST, Address#1210 ASC NULLS FIRST, Latlong#1211 ASC NULLS FIRST, Descript_index#1720 ASC NULLS FIRST, DayOfWeek_index#1735 ASC NULLS FIRST, PdDistrict_index#1751 ASC NULLS FIRST, Resolution_index#1768 ASC NULLS FIRST, Address_index#1786 ASC NULLS FIRST, Descript_ohe#1822 ASC NULLS FIRST, DayOfWeek_ohe#1823 ASC NULLS FIRST, PdDistrict_ohe#1824 ASC NULLS FIRST, Resolution_ohe#1825 ASC NULLS FIRST, Address_ohe#1826 ASC NULLS FIRST, Category_index#1877 ASC NULLS FIRST], false\n                        +- Project [Category#1205, Descript#1206, DayOfWeek#1207, PdDistrict#1208, Resolution#1209, Address#1210, Latlong#1211, Descript_index#1720, DayOfWeek_index#1735, PdDistrict_index#1751, Resolution_index#1768, Address_index#1786, Descript_ohe#1822, DayOfWeek_ohe#1823, PdDistrict_ohe#1824, Resolution_ohe#1825, Address_ohe#1826, UDF(cast(Category#1205 as string)) AS Category_index#1877]\n                           +- Project [Category#1205, Descript#1206, DayOfWeek#1207, PdDistrict#1208, Resolution#1209, Address#1210, Latlong#1211, Descript_index#1720, DayOfWeek_index#1735, PdDistrict_index#1751, Resolution_index#1768, Address_index#1786, UDF(cast(Descript_index#1720 as double), 0) AS Descript_ohe#1822, UDF(cast(DayOfWeek_index#1735 as double), 1) AS DayOfWeek_ohe#1823, UDF(cast(PdDistrict_index#1751 as double), 2) AS PdDistrict_ohe#1824, UDF(cast(Resolution_index#1768 as double), 3) AS Resolution_ohe#1825, UDF(cast(Address_index#1786 as double), 4) AS Address_ohe#1826]\n                              +- Project [Category#1205, Descript#1206, DayOfWeek#1207, PdDistrict#1208, Resolution#1209, Address#1210, Latlong#1211, Descript_index#1720, DayOfWeek_index#1735, PdDistrict_index#1751, Resolution_index#1768, UDF(cast(Address#1210 as string)) AS Address_index#1786]\n                                 +- Project [Category#1205, Descript#1206, DayOfWeek#1207, PdDistrict#1208, Resolution#1209, Address#1210, Latlong#1211, Descript_index#1720, DayOfWeek_index#1735, PdDistrict_index#1751, UDF(cast(Resolution#1209 as string)) AS Resolution_index#1768]\n                                    +- Project [Category#1205, Descript#1206, DayOfWeek#1207, PdDistrict#1208, Resolution#1209, Address#1210, Latlong#1211, Descript_index#1720, DayOfWeek_index#1735, UDF(cast(PdDistrict#1208 as string)) AS PdDistrict_index#1751]\n                                       +- Project [Category#1205, Descript#1206, DayOfWeek#1207, PdDistrict#1208, Resolution#1209, Address#1210, Latlong#1211, Descript_index#1720, UDF(cast(DayOfWeek#1207 as string)) AS DayOfWeek_index#1735]\n                                          +- Project [Category#1205, Descript#1206, DayOfWeek#1207, PdDistrict#1208, Resolution#1209, Address#1210, Latlong#1211, UDF(cast(Descript#1206 as string)) AS Descript_index#1720]\n                                             +- Relation [Category#1205,Descript#1206,DayOfWeek#1207,PdDistrict#1208,Resolution#1209,Address#1210,Latlong#1211] csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "predictions.groupBy('Category_index', 'prediction').count().show()\n",
    "\n",
    "# Calculate the elements of the confusion matrix\n",
    "TN = predictions.filter('prediction = 0 AND Category_index  = prediction').count()\n",
    "TP = predictions.filter('prediction = 1 AND Category_index  = prediction').count()\n",
    "FN = predictions.filter('prediction = 0 AND Category_index  = 1').count()\n",
    "FP = predictions.filter('prediction = 1 AND Category_index  = 0').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 149.0 failed 1 times, most recent failure: Lost task 0.0 in stage 149.0 (TID 526) (DESKTOP-AG0MV7U executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 683, in main\nRuntimeError: Python in worker has different version 3.9 than that in driver 3.10, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:179)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:179)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 683, in main\nRuntimeError: Python in worker has different version 3.9 than that in driver 3.10, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:179)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Usuario\\Documents\\Universidad\\BigData\\Crime-Classification-using-PySpark\\classification.ipynb Celda 25\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Universidad/BigData/Crime-Classification-using-PySpark/classification.ipynb#Y100sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m metrics \u001b[39m=\u001b[39m MulticlassMetrics(predictionAndLabels)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Universidad/BigData/Crime-Classification-using-PySpark/classification.ipynb#Y100sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Confusion Matrix\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Universidad/BigData/Crime-Classification-using-PySpark/classification.ipynb#Y100sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m cm \u001b[39m=\u001b[39m metrics\u001b[39m.\u001b[39mconfusionMatrix()\u001b[39m.\u001b[39mtoArray()\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\mllib\\evaluation.py:288\u001b[0m, in \u001b[0;36mMulticlassMetrics.__init__\u001b[1;34m(self, predictionAndLabels)\u001b[0m\n\u001b[0;32m    286\u001b[0m sc \u001b[39m=\u001b[39m predictionAndLabels\u001b[39m.\u001b[39mctx\n\u001b[0;32m    287\u001b[0m sql_ctx \u001b[39m=\u001b[39m SQLContext\u001b[39m.\u001b[39mgetOrCreate(sc)\n\u001b[1;32m--> 288\u001b[0m numCol \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(predictionAndLabels\u001b[39m.\u001b[39;49mfirst())\n\u001b[0;32m    289\u001b[0m schema \u001b[39m=\u001b[39m StructType(\n\u001b[0;32m    290\u001b[0m     [\n\u001b[0;32m    291\u001b[0m         StructField(\u001b[39m\"\u001b[39m\u001b[39mprediction\u001b[39m\u001b[39m\"\u001b[39m, DoubleType(), nullable\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m),\n\u001b[0;32m    292\u001b[0m         StructField(\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m, DoubleType(), nullable\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m),\n\u001b[0;32m    293\u001b[0m     ]\n\u001b[0;32m    294\u001b[0m )\n\u001b[0;32m    295\u001b[0m \u001b[39mif\u001b[39;00m numCol \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m3\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\rdd.py:2869\u001b[0m, in \u001b[0;36mRDD.first\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2843\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfirst\u001b[39m(\u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mRDD[T]\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m   2844\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2845\u001b[0m \u001b[39m    Return the first element in this RDD.\u001b[39;00m\n\u001b[0;32m   2846\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2867\u001b[0m \u001b[39m    ValueError: RDD is empty\u001b[39;00m\n\u001b[0;32m   2868\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2869\u001b[0m     rs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtake(\u001b[39m1\u001b[39;49m)\n\u001b[0;32m   2870\u001b[0m     \u001b[39mif\u001b[39;00m rs:\n\u001b[0;32m   2871\u001b[0m         \u001b[39mreturn\u001b[39;00m rs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\rdd.py:2836\u001b[0m, in \u001b[0;36mRDD.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   2833\u001b[0m         taken \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   2835\u001b[0m p \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(partsScanned, \u001b[39mmin\u001b[39m(partsScanned \u001b[39m+\u001b[39m numPartsToTry, totalParts))\n\u001b[1;32m-> 2836\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontext\u001b[39m.\u001b[39;49mrunJob(\u001b[39mself\u001b[39;49m, takeUpToNumLeft, p)\n\u001b[0;32m   2838\u001b[0m items \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m res\n\u001b[0;32m   2839\u001b[0m partsScanned \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m numPartsToTry\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:2319\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   2317\u001b[0m mappedRDD \u001b[39m=\u001b[39m rdd\u001b[39m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[0;32m   2318\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2319\u001b[0m sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonRDD\u001b[39m.\u001b[39;49mrunJob(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsc\u001b[39m.\u001b[39;49msc(), mappedRDD\u001b[39m.\u001b[39;49m_jrdd, partitions)\n\u001b[0;32m   2320\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 149.0 failed 1 times, most recent failure: Lost task 0.0 in stage 149.0 (TID 526) (DESKTOP-AG0MV7U executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 683, in main\nRuntimeError: Python in worker has different version 3.9 than that in driver 3.10, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:179)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:179)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 683, in main\nRuntimeError: Python in worker has different version 3.9 than that in driver 3.10, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:179)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = metrics.confusionMatrix().toArray()\n",
    "\n",
    "# True Positives\n",
    "TP = cm[0][0]\n",
    "\n",
    "# True Negatives\n",
    "TN = cm[1][1]\n",
    "\n",
    "# False Positives\n",
    "FP = cm[0][1]\n",
    "\n",
    "# False Negatives\n",
    "FN = cm[1][0]\n",
    "\n",
    "# Print out the results\n",
    "print(f\"True Positives: {TP}\")\n",
    "print(f\"True Negatives: {TN}\")\n",
    "print(f\"False Positives: {FP}\")\n",
    "print(f\"False Negatives: {FN}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
